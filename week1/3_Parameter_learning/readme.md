![](https://github.com/dineshsonachalam/Machine-Learning-Stanford/blob/master/week1/3_Parameter_learning/gradient_descent_1.png)



![](https://github.com/dineshsonachalam/Machine-Learning-Stanford/blob/master/week1/3_Parameter_learning/quiz.png)
***

![](https://github.com/dineshsonachalam/Machine-Learning-Stanford/blob/master/week1/3_Parameter_learning/gradient_descent_2.png)


***
![](https://github.com/dineshsonachalam/Machine-Learning-Stanford/blob/master/week1/3_Parameter_learning/quiz2.png)

***
![](https://github.com/dineshsonachalam/Machine-Learning-Stanford/blob/master/week1/3_Parameter_learning/gradient_descent_for%20_linear_regression_3.png)

***
***
# Learning with gradient descent:
What we would like is an algorithm which lets us find weights and biases so that the output from the network approximates y for all training input x. To quantify how well we are achieving this goal we define a cost function C(θ,b)=(1/2n)∑(ŷ-y)2,where ŷ is output generated by the network and y is the actual output, this cost function is sometimes called as **mean squared error**. Inspecting this cost function we see that C(θ,b) is non negative, since every term in the sum is non negative. Furthermore, the cost reduced is (C(θ,b) tends to 0), precisely when ŷ is approximately equal to y for all training input x. So our training algorithm has done a good job if it can find weight and biases so that C(θ,b) is minimized. If C(θ,b) is large our algorithm is not doing well. So the aim of our training algorithm will be to minimize the cost of C(θ,b) as a function of weights and biases. **In simple words, we want to find a set of weights and biases which make the cost as small as possible.** We will do that using an algorithm call gradient descent.

***

# Gradient descent:

For different values of ŷ , we get different values of the cost function.

![](https://everythingai.files.wordpress.com/2018/01/2-1-e1515252523396.png?w=768)

_Plot between cost function and predicted output._

We have to find ŷ for which cost function is minimum. One way to do that is bruteforce,i.e. Trying different values in the hope to find the best one for which the cost is minimum, which is not practically possible (is computational cost and time is very high).
So we will try gradient descent which is very faster than this.

Now let’s say we start from a point randomly and then we will calculate the slope of the line at that point.
![](https://everythingai.files.wordpress.com/2018/01/2-2-e1515252253337.png?w=768)

_Initialized with the point randomly_

If the slope is negative we will go right and if it is positive we will go left.

![](https://everythingai.files.wordpress.com/2018/01/2-3-e1515252721342.png)

_Calculate the slope._

Doing this repeatedly we will find the best value which minimizes the cost function.

![](https://everythingai.files.wordpress.com/2018/01/2-4-e1515252667761.png)

_Minimizing C with Gradient descent_

You can imagine this as a ball. Gradient Descent is called as gradient descent because we are deciding to the minimum of the cost function.

![](https://everythingai.files.wordpress.com/2018/01/2-5-e1515252843486.png)

_Gradient Descent For a 3D dataset and it’s 2D representation_

There’s more to gradient descent, but for intuition it is more than sufficient.

